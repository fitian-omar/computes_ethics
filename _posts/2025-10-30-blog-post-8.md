---
title: 'BIAS Blog 2: Harms in Machine Learning'
date: 2025-10-30
permalink: /posts/2025/10/blog-post-8/
tags:
 - harm
 - ethics
 - machine learning
---

The case study outlines sources of harm in machine learning throughout its data collection, modelling, evaluation, and deployment, and the unique treatment of such harms. I appreciated the typography of harm presented, which is definitely useful for my [case study](/computes_ethics/pages/casestudy.md/).

I've encountered many biases from algorithms and AI. For example, the job seekers I worked with over the summer had to overcome measurement bias when indicators of employability focused on US education and formal experience. This is confounded by deployment bias when recruiters give the model more weight than it deserves. Within the organisation, we would apply rules to clients equally, like having a limit on gift card assistance; this aggregation bias ignored unequal needs and subgroups.

We also encountered representation bias when conducting our satisfaction survey as it was easier to get clients who came in weekly to fill it out. It underrepresented clients with little time, infrequent meetings, or poor tech literacy (we also sent the survey over email).

In credit scoring, historical patterns can influence applicants, as the model will take in structural discrimination as financial irresponsibility. 

Qualitative research can help address representation bias, detecting subgroups and their unique qualities by focusing on individuals. This is similar to the logic of using group metrics rather than simply the aggregate to mitigate aggregation bias. It is important to understand the validity and ethics of a concept before focusing on measuring it. 

If I were to create a checklist to anticipate or prevent future harms I would first define the decision and affected stakeholders, identifying  outcomes and why they matter. I would then investigate whether the label is a proxy and if it varies by subgroup. If yes, I would fix the measurement. Then, I would check representation, comparing target, development, and use populations. Then, I would choose between one model or decoupled. I would keep the design people-centric by conducting interviews and thinking of the wellbeing of the user. 

My new discussion question: Can transparency alone correct algorithmic harm?

Tranparency like public documentation is necessary but insufficient to address harm. They can reveal where bias exists but maintain power dynamics. While we know that credit scores disadvantage people, and can understand how the algorithms work, we cannot change them. Therefore, transparency needs action, like ways to appeal, participatory model governance, and the right to be an exclusion from data-driven rule. So we need to move from only disclosure towards co-ownership.

This reading sharpened my vocabulary by providing a typography of bias with unique remedies. The allocative/representational lens also reframed my nonprofit experience; itâ€™s not only about who gets a resource, but also about whose narratives are erased by our benchmarks. This made me think more critically about study design and data-driven rules.