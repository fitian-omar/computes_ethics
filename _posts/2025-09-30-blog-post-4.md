---
title: 'Blog 4: How GenAI Works'
date: 2025-09-25
permalink: /posts/2025/09/blog-post-4/
tags:
 - case study
 - genai
 - ethics
---


Learning and machine bias

The case study explains how modern generative AI, or simply gen AI, works "under the hood." It highlights the strengths, limitations, and biases of the models and discusses issues of ethics and society.

Large language models (LLMs) do not reason as presented by popular media. They are probabilistic machines that are trained to predict the most likely next token given local context, not unlike autocomplete. While earlier studies argue for emergent abilities like math or multi-step reasoning, articles like Schaeffer et al (2023) argue that this is a mirage resulting from the choice of research metrics.

LLMs do not genuinely reason because they are built on statistical pattern matching and sequence prediction not logical comprehension. The transfomer model is built to predict the next token, which is achieved through massive amounts of data. The model uses the data to predict the probability of the next token. Transfomers are built by minimising the cross-entropy, a measure of the model's predicted probability distribution to the actual next token in the training data. With millions of tokens and large number of calculations, the model learns how to create complex linguistic patterns that is pattern matching. As parameters incerase, the per-token error falls, especially if measured with a linear or continuous metric. Improvements in reasoning look abrupt because of the ways model output is measured. For example

Abrupt 

New Discussion Question

If LLMs simply predict the likelihood of words, how does the training data influence their blind spots and biases?

Since models are incapable of comprehending texts, they are limited to their training corpus. A model built on internet texts would generate common language, and from language, ideas. This is also presents the problem of cultural and linguistic bias. Additionally, even if we were to incorporate texts from other languages to enrich the model's faux "understanding," there is no clear process of evaluating the translations themselves.

I enjoyed reading the article because it explained the complexicty of AI in an approachable way. I am not confident of my understanding of the research articles, but I found their takeaways helpful for a holistic view of gen AI. This exercise made me further committed to governing LLM use and advocating for critical engagement and bookkeeping.