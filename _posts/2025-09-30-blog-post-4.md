---
title: 'Blog 4: How GenAI Works'
date: 2025-09-25
permalink: /posts/2025/09/blog-post-4/
tags:
 - case study
 - genai
 - ethics
---


Learning and machine bias

The case study explains how modern generative AI, or simply gen AI, works "under the hood." It highlights the strengths, limitations, and biases of the models and discusses issues of ethics and society.

Large language models (LLMs) do not reason as presented by popular media. They are probabilistic machines that are trained to predict the most likely next token given local context, not unlike autocomplete. While earlier studies argue for emergent abilities like math or multi-step reasoning, articles like Schaeffer et al (2023) argue that this is a mirage resulting from the choice of research metrics. 

Because most people are not familiar with how these models are built, it is importatnt to clarify that commercial LLMs are are mostly trained using a specific architecture called a transformer model. LLMs do not genuinely reason because they are built on statistical pattern matching and sequence prediction not logical comprehension. The transfomer model is built to predict the next token in a sequence, which is achieved through massive amounts of data. The model uses the data to predict the probability of the next token. Training works by minimising "cross-entropy loss" which measures the difference between what the model predicted and what the correct next token actually was in the training data.

With millions of tokens and large number of calculations, the model learns how to create complex linguistic patterns that is pattern matching. As parameters incerase, the per-token error falls, especially if measured with a linear or continuous metric. This connects to evaluation metrics, which are ways of scoring how well a model performs on a task. Improvements in reasoning look abrupt because of the ways model output is measured. For example, certain evaluation metrics like exact string match or "all digits must be correct" make the gradual accuracy of models seem like it has abrupt peaks. So, LLMs simulate reasoning by regurgitating patterns of speech. They cannot reason, for example, I've had to revise AI-generated questions to remove leading questions.

Given all of this, I wonder if LLMs simply predict the likelihood of words, how does the training data influence their blind spots and biases?

Since LLMs are built on pattern recognition rather than understanding, their limitations are inherited from their training data. A model built on internet texts would generate common language, and from language, ideas. This is also presents the problem of cultural and linguistic bias. Additionally, even if we were to incorporate texts from other languages to enrich the model's faux "understanding," there is no clear process of evaluating the translations themselves. I like to evaluate translations of Arabic literature. With a small Arabic corpus, I don't trust that models have the capabality let alone reason to arrive at proper interpretations or translations of Arabic content. 

I enjoyed reading the article because it explained the complexicty of AI in an approachable way. I am not confident of my understanding of the research articles, but I found their takeaways helpful for a holistic view of gen AI. This exercise made me further committed to governing LLM use and advocating for critical engagement and bookkeeping.