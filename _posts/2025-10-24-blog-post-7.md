---
title: 'Blog 7: Right to Fair Representation'
date: 2025-10-24
permalink: /posts/2025/09/blog-post-7/
tags:
 - case study
 - ethics
---

The case study evaluates how text to image models depict South Asian cultures. It argues that these models fail to recognise cultural concepts, are biased towards defaults, and perpetuate cultural tropes, calling for community-based evaluations in addition to standardised evaluations.

Cultural representation is important in preserving specificity without collapsing it under other categories. I also think the impulse to preserve culture is a symptom of collective narcissism. I think cultural representation should only be important in the context of strategic essentialism, the view that abject subjects must organise for their liberation under an organising principle like culture or identity. The models show an Orientalist lens, which historically has been used for imperial purposes, so attention to power relations is also important. I would evaluate outputs by the extent to which they acknowledge the social construction of identity and culture, as both are ossified in the act of defining and borders between self and other are inherently exclusionary. I would also evaluate if the specificity of concepts is preserved and accurately translated.

While benchmarks can give an aggregate indicator of progress, they fail to elaborate on the extent of cultural harm. Lived experience like the erasure of Bangla scripts carry harm that is missed by benchmarks.

AI can be more globally inclusive provided that engineers dialogue with communities and include more texts from marginalised voices. I also think that local engineers should also participate in creating these models, increasing the cultural sensitivity of AI while also decentralising the tech economy.

Developers need to engage with the corpus of the communities they wish to include, which encompasses their grand narratives and cultural criticism to govern their project. They must also collect more robust data from sub-regions and locales in an effort to disaggregate, and add qualitiative harm checks.

Encoding freezes in time the organic process of culture, so regular updates probably through direct engagement with the cultural production of the community can keep the dataset relevant while archiving its developments. It must also be aware of the level of aggregation it operates at. Cultures are heterogeneous, and awareness of subregions is important. Perhaps the addition of metadata like city, neighborhood, or perhaps even coordinates can somewhat help in avoiding aggregating lived experience into a larger concept.

Due to its indexical quality, film and photography have a large democratic potential, but the act of seeing is socially constructed (take for example photographers on a farm, males photographed buildings and tools while women photographed animals and children) and as seen in this case study, the data on which models are built can be biased. Therefore we should audit the data we feed our models.

My discussion question is, who should be responsible to correct cultural misrepresentations?

This is an interesting question because although I advocate for direct contact with communities, or perhaps disregarding this voyueristic desire to study and assimilate their cultures, contact is often difficult especially across spatial displacement, power relations, borders, and linguistic barriers. This means that diasporas and elites, who are more likely to be accessible could hold the position to criticise cultural misuse, but who is to evaluate theirs? It's an important question because it relates to the cultural imperialism of the majority.

I liked this exercise because it is directly at the intersection of my fields of study: computation and social science. It's interseting to know how the technical aspect of AI works and the interpretive and diffuse nature of culture.