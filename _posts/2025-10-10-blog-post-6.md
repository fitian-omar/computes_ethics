---
title: 'Blog 6: Implications of a Tech Focused Society'
date: 2025-10-07
permalink: /posts/2025/09/blog-post-6/
tags:
 - case study
 - ethics
 - society
 - ai
 - companionship
---

AI and alienation

**Article**  
[Addictive Intelligence: Understanding Psychological, Legal, and Technical Dimensions of AI Companionship](https://mit-serc.pubpub.org/pub/iopjyxcx/release/2?readingCollection=132bb7af)

The case study argues that AI companions produce psychological dependence through sycophancy and hyper-personalisation. It presents examples of recent harm to argue for corrective policies.

How can companies design AI companions to be emotionally engaging while preventing harmful psychological dependencies? 

I reject the premise that AI should be emotionally engaging. The demand for emotional support by AI, most common among economically or socially marginalised groups, is a symptom of the erosion of social ties. The problem worthy of address is not AI design but alientation itself. To ignore the context of this demand is to normalise its cause.

However, assuming for the sake of this assignment that the demand is not caused by alienation, I would suggest creating well-being indicators from AI usage data. The platform, with transparency of its care priorities, can monitor user tone, duration and frequency of contact, and can seek expert human intervention during crises. I would also suggest modifying AI parameters to avoid romantic scenarios. Despite these considerations commercial AI may be subject to, people may still be able to manipulate AI or create their own without these safeguards.

How does addiction to AI companions compare with other forms of technology addiction, such as social media or gaming?

All three optimise engagement but AI companions "shapeshift through a kaleidoscope of characters, personalities, and mannerisms to find the precise combination to keep users engaged," armed with enormous data from human affective experiences. They can also imitate admired personas like celebrities or fictional characters, which can subconsciously create a sense of intimacy despite knowledge of its artificiality. Although AI content has been growing, social media  content is mostly user-generated which can produce imperfect viewing experiences, albeit subject to algorithmic curation. AI companions will rarely produce such dissonance. Although it remains highly addictive, gaming requires loss missing from AI companions.

An elderly person finds genuine comfort in an AI companion, alleviating their loneliness, but their family worries this relationship is replacing real human connections. How should we evaluate the benefits versus risks in such cases? What ethical guidelines or intervention strategies might help determine when AI companionship crosses from beneficial to harmful?

Integrate the aforementioned usage data well-being indicators, perhaps monitored with consent by their families. The qustion also depends on what is considered beneficial or harmful. Can elders continue use if they develop delusions but are happy? If I say no, then literature-backed psychic evaluation should be conducted. However, this is problematic due to normative pathology. I would suggest providing elders with aesthetic dosages administered by humans to alleviate their loneliness, alienation.

Current business models incentivize AI companies to maximize user engagement. What alternative economic models could promote healthier AI interactions while maintaining commercial viability?

Ethical AI certifications can be especially helpful if designed and disseminated through a disintersted and expert authority. However, Big Tech's lobbying and the current illusion of regulation make any prospective policy questionable at best.

How would you address age restrictions, usage limits, and safety monitoring while respecting user privacy and autonomy?

There are privacy concerns associated with age verifications despite easing age-appropriate AI engagement. Therefore, I think it is a parent's duty to monitor what their child is exposed to. I would not offer paternalistic usage limits. I would add well-being suggestions and opt-in expert human intervention during crises. I would anonymise all data and allow data deletion. Sewell's case could have prevented by blocking the romanticisation of self-harm, escalating to a crisis hotline, or notifying a guardian, but I maintain that the parents should have been more vigilant.

Question: Would the demand for companion AI exist in a non-alienated society?

The paper highlights the tension between respect for autonomy and the ethical duty to prevent dependency. However, the very demand of companion AI is a product of social alientation. In a non-alienated society, would this demand still arise? I suppose people would still experience loss and isolation and assuaging these emotions through AI is not inherently harmful.

The article strengthened my distaste for companion AI and elaborated on its technical language. I was not aware of the psychology behind companion AI addiction. It was insightful to think about how problematic use cases could be prevented and handled. I realised that I value autonomy, privacy, and organic human interaction. It was sad to hear about Sewell.