---
title: 'Blog 2: Crafting Arguments'
date: 2025-09-21
permalink: /posts/2025/09/blog-post-2/
tags:
  - case study
  - ethics
  - arguments
---

The demand for military AI.

**News Article**  
[Peter Thiel’s Palantir is seeing ‘unprecedented’ demand for its military A.I. that its CEO calls ‘a weapon that will allow you to win’](https://fortune.com/2023/05/09/peter-thiel-palantir-unprecedented-demand-ai-artificial-intelligence/)

The article presents Thiel’s claims in ways that normalise war as a marketplace and dangerous AI as just another commodity. It reports that Palantir is experiencing "unprecedented demand" and quotes the CEO describing it as "a weapon that will allow you to win." Early customer rollouts are presented as evidence of a natural trend toward adoption. Although the article does not explicitly argue that militaries should adopt Palantir's tools, the framing and lack of criticism makes that conclusion feel obvious. Furthermore, it treats the rise of AI demand as a neutral fact of the market, an object that simply exists and justifies itself, rather than as as political, ethical, and ultimately human decision actively and constantly made by individuals.

A major issue in the article's reasoning is a form of reification, which in this context is the fallacy of treating an abstract social process (like militarisation and market demand) as if it were an independent, objective force. When the article says there "unprecedented demand" it presents demands as an autonomous agent that compels adoption. However, demand is not a natural phenomenon but one constrcuted by geopolitics, marketing, and state investment in warfare (thesmelves justified by the erroneous belief of killing in order to live).

Reification occurs when human actions and social relations come to appear as thing-like necessities rather than choices, obscuring the ethical stakes behind them. By presenting militarisation as an inevitable market trend, the article normalises the expansion of weapons technologies as if they were simply prodcuts responding to consumer interest.

Reification explains Palantir’s eagerness to “aggressively capitalize” as nation states militarise increasingly. This is obviously a fascist sentiment. The military requires a constructed, malicious Other; ideology conjures scarcity to justify bruteness. This violates nature and care ethics where human wellbeing is relegated to profit. Furthermore, it breaches conractarian ethics in which the social contract itself is unjust, as well as virtue ethics, in which people are dehumanised and made into collateral.

This matters because it commodifies suffering, as automated targeting systems risk reducing human beings to data points, eroding moral and legal protections for civlians. Treating human life as an output variable is dehumanising where violence becomes abstracted behind dehumanising epithets (e.g. targets neutralised, collateral damage—you can also argue that nationalities, like German, American, South African, are also forms of dehumanising language). The quanitfication of death is also the problem with a utilitarian lens, where it becomes a matter of arithmetic in choosing the "lesser evil." Humanity should be the end, not the means; that is why one death is too many and suffering is not comparable to other suffering. When the article frames Palantir's platform as a tool to "win," it relies on the abstraction and erasue of humans withing that "triumph."

The article also fails to contextualise military technology within the history of the defense industry, which actively lobbies to manufacture state adoption. 

Palantir’s AI should not be adopted for military use. While the article portrays adoption as natural, ethical analysis shows that this technology violates human dignity, entrenches unjust power imbalances, and risks massive harm. Military AI like Palantir’s turns life and death into optimisable metrics. If we accept demand as destiny, we accept that war is a marketplace and that civilians are collateral outputs. This is why militaries should not adopt Palantir’s AI. Ethical analysis shows that the technology undermines human dignity, entrenches unjust power dynamics, and enables large-scale harm under the guise of innovation.

This exercise made me realise how the same reasoning can be analysed in different ways. On the surface, the fallacy is a logical one: treating “demand” as an autonomous justification. But digging deeper, it reflects how capitalism commodifies life itself, turning suffering into a metric and war into a marketplace.